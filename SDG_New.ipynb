{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f628c31",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">MDSC-302 Assignment</h1>\n",
    "<h3 align=\"right\">Registration number: 20233</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b6355f",
   "metadata": {},
   "source": [
    "# Sustainable Development Goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "904e52bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from typing import Any, Iterable, List, Tuple, Union, NoReturn\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ff14a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gunta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gunta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Downlading nltk stopwords and punkt \n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a46c4778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enabling extensions\n",
    "#!jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
    "#!jupyter serverextension enable voila --sys-prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21df1b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-gram matcher -(From OSDG github)\n",
    "class NgramMatcher:\n",
    "    def __init__(self,\n",
    "                 ngrams: Iterable[str],\n",
    "                 lowercase: bool = True,\n",
    "                 token_pattern: str = r'(?u)\\b\\w+\\b',\n",
    "                 ngram_size: Union[int, Tuple[int, int]] = (1, 1)):\n",
    "        r\"\"\"\n",
    "        Ngram matcher and counter.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ngrams : Iterable[str]\n",
    "            List of ngrams to match.\n",
    "        lowercase : bool, by default True\n",
    "            Lowercases all characters.\n",
    "        token_pattern : str, by default r'(?u)\\b\\w+\\b'\n",
    "            Regex expression that designates a token.\n",
    "        ngram_size : Union[int, Tuple[int, int]], by default (1, 1)\n",
    "            Minimum and maximum boundaries for token amount per ngram.\n",
    "            If integer is provided, it represents both minimum and maximum boundaries.\n",
    "        \"\"\"\n",
    "        self.__validate_ngrams(ngrams)\n",
    "        self.ngrams = np.array(ngrams)\n",
    "\n",
    "        self.lowercase = lowercase\n",
    "        if lowercase:\n",
    "            self.ngram_index_map = {ngram.strip().lower(): idx for idx, ngram in enumerate(ngrams)}\n",
    "        else:\n",
    "            self.ngram_index_map = {ngram.strip(): idx for idx, ngram in enumerate(ngrams)}\n",
    "\n",
    "        self.token_pattern = token_pattern\n",
    "        self.__token_pattern = re.compile(token_pattern)\n",
    "        if self.__token_pattern.groups > 1:\n",
    "            raise ValueError(f'Too many groups in ngram pattern : {self.__token_pattern.groups}')\n",
    "\n",
    "        if isinstance(ngram_size, int):\n",
    "            self.ngram_size = (ngram_size, ngram_size)\n",
    "        elif isinstance(ngram_size, tuple) and len(ngram_size) == 2:\n",
    "            self.ngram_size = ngram_size\n",
    "        else:\n",
    "            raise ValueError(f'Expected int or tuple of length 2 for argument ngram_size. Got {type(ngram_size)}.')\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def __validate_ngrams(ngrams) -> NoReturn:\n",
    "        if isinstance(ngrams, str) or not hasattr(ngrams, '__iter__'):\n",
    "            raise ValueError('Terms must be iterable.')\n",
    "        if len(ngrams) == 0:\n",
    "            raise ValueError('Empty terms passed.')\n",
    "        if len(ngrams) != len(set(ngrams)):\n",
    "            raise ValueError('Terms contain duplicate entries.')\n",
    "        try:\n",
    "            _ = ''.join(ngrams)\n",
    "        except TypeError:\n",
    "            raise TypeError('Terms contain non str type values.')\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def __validate_documents(documents: Any) -> NoReturn:\n",
    "        if isinstance(documents, str) or not hasattr(documents, '__iter__'):\n",
    "            raise TypeError('Iterable of strings is expected.')\n",
    "        if any(not isinstance(doc, str) for doc in documents):\n",
    "            raise TypeError('Documents contain non str values.')\n",
    "\n",
    "\n",
    "    def _generate_ngrams(self, document: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extracts ngrams from text.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        document : str\n",
    "            Text\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[str]\n",
    "            List of generated ngrams.\n",
    "        \"\"\"\n",
    "        if self.lowercase:\n",
    "            document = document.lower()\n",
    "        tokens = self.__token_pattern.findall(document)\n",
    "\n",
    "        min_n, max_n = self.ngram_size\n",
    "        if max_n == 1:\n",
    "            return tokens\n",
    "\n",
    "        if min_n == 1:\n",
    "            ngrams = list(tokens)\n",
    "            min_n += 1\n",
    "        else:\n",
    "            ngrams = list()\n",
    "\n",
    "        n_tokens = len(tokens)\n",
    "\n",
    "        for k in range(min_n, min(max_n + 1, n_tokens + 1)):\n",
    "            for j in range(n_tokens - k + 1):\n",
    "                ngrams.append(\n",
    "                    ' '.join(tokens[j:j+k])\n",
    "                )\n",
    "\n",
    "        return ngrams\n",
    "\n",
    "\n",
    "    def _match_ngrams(self, documents: Iterable[str]) -> List[Tuple[List[int], List[int]]]:\n",
    "        \"\"\"\n",
    "        Matches ngrams to texts.\n",
    "        For each document:\n",
    "          1. Converts document into tokens\n",
    "          2. Generates ngrams of size defined in ngram_size\n",
    "          3. Crossreferences ngrams with matching ngrams\n",
    "          > Counts each ngram frequency\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        documents : Iterable[str]\n",
    "            List of texts.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[Tuple[List[int], List[int]]]\n",
    "            Each element is a tuple.\n",
    "              - index 0 contains ngram indices List[int]\n",
    "              - index 1 contains ngram frequencies List[int]\n",
    "        \"\"\"\n",
    "        ngrams = []\n",
    "\n",
    "        ngram_index_map = self.ngram_index_map\n",
    "\n",
    "        self.__validate_documents(documents)\n",
    "        for document in documents:\n",
    "            ngram_counts = dict()\n",
    "            for ngram in self._generate_ngrams(document):\n",
    "                try:\n",
    "                    idx = ngram_index_map[ngram]\n",
    "                    try:\n",
    "                        ngram_counts[idx] += 1\n",
    "                    except KeyError:\n",
    "                        ngram_counts[idx] = 1\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "            ngrams.append((list(ngram_counts.keys()),\n",
    "                           list(ngram_counts.values())))\n",
    "\n",
    "        return ngrams\n",
    "\n",
    "\n",
    "    def match(self, documents: Iterable[str]) -> List[Tuple[List[int], List[int]]]:\n",
    "        \"\"\"\n",
    "        Matches ngrams to texts.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        documents : Iterable[str]\n",
    "            List of texts.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[Tuple[List[int], List[int]]]\n",
    "            Each element is a tuple.\n",
    "              - index 0 contains ngram indices List[int]\n",
    "              - index 1 contains ngram frequencies List[int]\n",
    "        \"\"\"\n",
    "        ngrams = self._match_ngrams(documents)\n",
    "        return ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91c02b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading fos_ids and fos_names \n",
    "fos_ids = np.load('fos_ids.npy', allow_pickle=True)\n",
    "fos_names = np.load('fos_names.npy', allow_pickle=True)\n",
    "\n",
    "# creating object for n-gram matcher\n",
    "ngram_matcher = NgramMatcher(fos_names,\n",
    "                             lowercase=True,\n",
    "                             token_pattern=r'(?u)\\b\\w+\\b',\n",
    "                             ngram_size=(1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c854667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for mapping fos_names to fos_ids\n",
    "def mapping_ids(text):\n",
    "    idxs, frequencies = ngram_matcher.match([text])[0]\n",
    "    ngrams = sorted(zip(fos_ids[idxs], fos_names[idxs], frequencies), key=lambda ng: len(ng[1]), reverse=True)\n",
    "    descored_ngrams = list()\n",
    "\n",
    "    for idx, (ngram_id, ngram_name, frequency) in enumerate(ngrams):\n",
    "        for _, fol_ngram_name, fol_frequency in ngrams[:idx]:\n",
    "            if ngram_name in fol_ngram_name:\n",
    "                frequency -= fol_frequency\n",
    "        \n",
    "        if frequency > 0:\n",
    "            descored_ngrams.append([ngram_id, ngram_name, frequency])\n",
    "        \n",
    "    ngrams = descored_ngrams\n",
    "    submerged_ngrams, drop_ngram_ids = list(), set()\n",
    "    \n",
    "    for idx, (ngram_id, ngram_name, frequency) in enumerate(ngrams):\n",
    "        for ngram_id2, ngram_name2, frequency2 in ngrams[idx+1:]:\n",
    "            if ngram_name2 in ngram_name:\n",
    "                frequency += frequency2\n",
    "                drop_ngram_ids.add(ngram_id2)\n",
    "        \n",
    "        submerged_ngrams.append([ngram_id, ngram_name, frequency])\n",
    "    submerged_ngrams = list(filter(lambda ng: ng[0] not in drop_ngram_ids, submerged_ngrams))\n",
    "\n",
    "    return {fos_id: frequency for fos_id, _, frequency in submerged_ngrams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7971475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting mappings of sdg and fos_ids\n",
    "with open('OSDG-mapping.json', 'r') as file_:\n",
    "    mapping = [(sdg, set(fos_ids)) for sdg, fos_ids in json.load(file_).items()]\n",
    "\n",
    "with open('OSDG-fosmap.json', 'r') as file_:\n",
    "    fosmap = json.load(file_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e61d316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess the text\n",
    "def preprocessText(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.strip()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [] \n",
    "  \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)\n",
    "    text_clean = (' '.join(filtered_sentence))\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "691c916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for mapping fos_ids to sdg's\n",
    "def sdgs_imp(fos):\n",
    "    sdgs = []\n",
    "    fos_ids = fos.keys()\n",
    "    \n",
    "    for sdg, sdg_fos_ids in mapping:\n",
    "        importance_fos_ids = sdg_fos_ids.intersection(fos_ids)\n",
    "        \n",
    "        if len(importance_fos_ids) >= 1:\n",
    "            importance = 0      \n",
    "            for fos_id in importance_fos_ids:\n",
    "                importance += fos.get(fos_id)\n",
    "                \n",
    "            sdgs.append({'SDG': sdg,\n",
    "                         'Importance': float(importance),\n",
    "                         'keywords': list(map(lambda fos_id: fosmap[fos_id], importance_fos_ids))})\n",
    "    \n",
    "    return sorted(sdgs, key=lambda x: x['Importance'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96d5237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for getting all sdg's\n",
    "def allsdg(sdgs):\n",
    "    if sdgs:\n",
    "        for sdg in sdgs:\n",
    "            print(sdg)\n",
    "    else:\n",
    "        print(\"No SDG's Found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b206efbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_text(path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        pdfFileObj = open(path, 'rb')\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        for page in range(0,pdfReader.numPages):\n",
    "            text = text + pdfReader.getPage(page).extractText()\n",
    "        pdfFileObj.close()\n",
    "    except:\n",
    "        return \"File not exsists\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71db71c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# widget for uploading a text file\n",
    "uploader = widgets.FileUpload(multiple=False, description = 'Upload text file')\n",
    "pdf_file = widgets.Text(style={'description_width': 'initial'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2f3e31",
   "metadata": {},
   "source": [
    "## Upload a text file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4713e3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a91fc5f875415d95926d73ad528b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, description='Upload text file')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(uploader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a75d756",
   "metadata": {},
   "source": [
    "## Give pdf file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa0fc777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93bb6cfde21d42468e98f428c451c514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "730fe793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10638f4533e44d839b3580b4cf660c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit', style=ButtonStyle(), tooltip='Send')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3d72c815954f66bf6373be77e46eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# widget for getting top sdg\n",
    "button_send = widgets.Button(\n",
    "                description='Submit',\n",
    "                tooltip='Send',\n",
    "                style={'description_width': 'initial'})\n",
    "\n",
    "# widget for output\n",
    "output = widgets.Output()\n",
    "\n",
    "# function to take action for widget button1\n",
    "def on_button_clicked(event):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        \n",
    "        if(uploader.value !={}):\n",
    "            print(\"SDG's from Text:\")\n",
    "            allsdg(sdgs_imp(mapping_ids(preprocessText(str(uploader.value)))))\n",
    "        else:\n",
    "            print(\"Text file not found\\n\")\n",
    "        if(pdf_file.value != ''):\n",
    "            print(\"SDG's from Pdf:\")\n",
    "            allsdg(sdgs_imp(mapping_ids(preprocessText(pdf_text(pdf_file.value)))))\n",
    "        else:\n",
    "            print(\"\\nPDF not found\")\n",
    "# Actions for corresponding buttons\n",
    "button_send.on_click(on_button_clicked)\n",
    "\n",
    "# Displaying the buttons\n",
    "display(button_send)\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d701c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
